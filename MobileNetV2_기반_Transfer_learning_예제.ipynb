{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2_기반 Transfer learning 예제.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhbale11/Object_Detection/blob/main/MobileNetV2_%EA%B8%B0%EB%B0%98_Transfer_learning_%EC%98%88%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybmX8rP-3qiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a95a84-1510-45de-b0ff-528a13f4034a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 17 03:52:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDnap1jLv8so"
      },
      "source": [
        "import os\n",
        "import os.path as pth\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffY3gSLAvvSs"
      },
      "source": [
        "BASE_MODEL_NAME = 'MobileNetV2-for-upload'\n",
        "my_model_base = keras.applications.mobilenet_v2\n",
        "my_model = my_model_base.MobileNetV2\n",
        "\n",
        "config = {\n",
        "    'is_zscore':True,\n",
        "    \n",
        "    # 'input_shape': (540, 960, 3),\n",
        "    'aug': {\n",
        "        'resize': (270, 480),\n",
        "    },\n",
        "    # 'input_shape': (224, 360, 3),\n",
        "    'input_shape': (270, 480, 3),\n",
        "\n",
        "    'output_activation': 'softmax',\n",
        "    'num_class': 1049,\n",
        "    'output_size': 1049,\n",
        "    \n",
        "    'conv':{\n",
        "        'conv_num': (0), # (3,5,3),\n",
        "        'base_channel': 0, # 4,\n",
        "        'kernel_size': 0, # 3,\n",
        "        'padding':'same',\n",
        "        'stride':'X'\n",
        "    },\n",
        "    'pool':{\n",
        "        'type':'X',\n",
        "        'size':'X',\n",
        "        'stride':'X',\n",
        "        'padding':'same'\n",
        "    },\n",
        "    'fc':{\n",
        "        'fc_num': 0,\n",
        "     },\n",
        "    \n",
        "    'activation':'relu',\n",
        "    \n",
        "    'between_type': 'avg',\n",
        "    \n",
        "    'is_batchnorm': True,\n",
        "    'is_dropout': False,\n",
        "    'dropout_rate': 0.5,\n",
        "    \n",
        "    'batch_size': 64,\n",
        "    'buffer_size': 256,\n",
        "    'loss': 'CategoricalCrossentropy',\n",
        "    \n",
        "    'num_epoch': 10000,\n",
        "    'learning_rate': 1e-3,\n",
        "    \n",
        "    'random_state': 7777\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abOw4s0jv6JI"
      },
      "source": [
        "image_feature_description = {\n",
        "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    'randmark_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    # 'id': tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "def _parse_image_function(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "\n",
        "def map_func(target_record):\n",
        "    img = target_record['image_raw']\n",
        "    label = target_record['randmark_id']\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.dtypes.cast(img, tf.float32)\n",
        "    return img, label\n",
        "\n",
        "def resize_and_crop_func(image, label):\n",
        "    result_image = tf.image.resize(image, config['aug']['resize'])\n",
        "    # result_image = tf.image.random_crop(image, size=config['input_shape'], seed=7777)\n",
        "    return result_image, label\n",
        "\n",
        "def image_aug_func(image, label):\n",
        "    pass\n",
        "    return image, label\n",
        "\n",
        "def post_process_func(image, label):\n",
        "    # result_image = result_image / 255\n",
        "    result_image = my_model_base.preprocess_input(image)\n",
        "    onehot_label = tf.one_hot(label, depth=config['num_class'])\n",
        "    return result_image, onehot_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1UN3LYJzFgd"
      },
      "source": [
        "data_base_path = pth.join('data', 'public')\n",
        "os.makedirs(data_base_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks1l_51cNzLP"
      },
      "source": [
        "category_csv_name = 'category.csv'\n",
        "category_json_name = 'category.json'\n",
        "submission_csv_name = 'sample_submisstion.csv'\n",
        "train_csv_name = 'train.csv'\n",
        "\n",
        "# train_zip_name = 'train.zip'\n",
        "train_tfrecord_name = 'all_train.tfrecords'\n",
        "train_tfrecord_path = pth.join(data_base_path, train_tfrecord_name)\n",
        "val_tfrecord_name = 'all_val.tfrecords'\n",
        "val_tfrecord_path = pth.join(data_base_path, val_tfrecord_name)\n",
        "# test_zip_name = 'test.zip'\n",
        "test_tfrecord_name = 'test.tfrecords'\n",
        "test_tfrecord_path = pth.join(data_base_path, test_tfrecord_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaBBHyX0dMig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "7726e305-023a-4ad2-f692-d52a87998708"
      },
      "source": [
        "train_csv_path = pth.join(data_base_path, train_csv_name)\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_dict = {k:v for k, v in train_df.values}\n",
        "\n",
        "submission_csv_path = pth.join(data_base_path, submission_csv_name)\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "# submission_df.head()\n",
        "\n",
        "category_csv_path = pth.join(data_base_path, category_csv_name)\n",
        "category_df = pd.read_csv(category_csv_path)\n",
        "category_dict = {k:v for k, v in category_df.values}\n",
        "# category_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2f9f0b9c9bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_csv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_csv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubmission_csv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_csv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/public/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJsUtTnqCQjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdng6pk8k0fH"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9-4T5OMcy1R"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GroupKFold, RepeatedStratifiedKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import os.path as pth\n",
        "import shutil\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from itertools import product, combinations\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "import datetime\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
        "                                    Flatten, Conv3D, AveragePooling3D, MaxPooling3D, Dropout, \\\n",
        "                                    Concatenate, GlobalMaxPool3D, GlobalAvgPool3D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
        "                                        EarlyStopping\n",
        "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKtUQ9mnKMn"
      },
      "source": [
        "conv_comb_list = []\n",
        "conv_comb_list += [(0,)]\n",
        "\n",
        "base_channel_list = [0]\n",
        "\n",
        "fc_list = [0] # 128, 0\n",
        "\n",
        "# between_type_list = [None, 'avg', 'max']\n",
        "between_type_list = ['avg']\n",
        "\n",
        "batch_size_list = [80]\n",
        "\n",
        "activation_list = ['relu']\n",
        "\n",
        "# len(conv_comb_list), conv_comb_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAaKPD3cnKB5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrRPrv1aoOEA"
      },
      "source": [
        "def build_cnn(config):\n",
        "    input_layer = Input(shape=config['input_shape'], name='input_layer')\n",
        "    pret_model = my_model(\n",
        "        input_tensor=input_layer, include_top=False, weights='imagenet', \n",
        "        input_shape=config['input_shape'], pooling=config['between_type'], \n",
        "        classes=config['output_size']\n",
        "    )\n",
        "\n",
        "    pret_model.trainable = False\n",
        "    \n",
        "    x = pret_model.output\n",
        "    \n",
        "    if config['between_type'] == None:\n",
        "        x = Flatten(name='flatten_layer')(x)\n",
        "        \n",
        "    if config['is_dropout']:\n",
        "        x = Dropout(config['dropout_rate'], name='output_dropout')(x)    \n",
        "            \n",
        "    x = Dense(config['output_size'], activation=config['output_activation'], \n",
        "          name='output_fc')(x)\n",
        "#     x = Activation(activation=config['output_activation'], name='output_activation')(x)\n",
        "    model = Model(inputs=input_layer, outputs=x, name='{}'.format(BASE_MODEL_NAME))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5mZ06q3qAmN"
      },
      "source": [
        "model = build_cnn(config)\n",
        "model.summary(line_length=150)\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCsZqqHyqAds"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CFjGGnr1iDN"
      },
      "source": [
        "origin_train_len = len(train_df) / 5 * 4\n",
        "origin_val_len = len(train_df) / 5 * 1\n",
        "\n",
        "train_num_steps = int(np.ceil((origin_train_len)/config['batch_size']))\n",
        "val_num_steps = int(np.ceil((origin_val_len)/config['batch_size']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzKzI0vXsrJp"
      },
      "source": [
        "model_base_path = data_base_path\n",
        "model_checkpoint_path = pth.join(model_base_path, 'checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta_kqsLstQcV"
      },
      "source": [
        "for conv_comb, activation, base_channel, \\\n",
        "    between_type, fc_num, batch_size \\\n",
        "        in itertools.product(conv_comb_list, activation_list,\n",
        "                              base_channel_list, between_type_list, fc_list,\n",
        "                              batch_size_list):\n",
        "    config['conv']['conv_num'] = conv_comb\n",
        "    config['conv']['base_channel'] = base_channel\n",
        "    config['activation'] = activation\n",
        "    config['between_type'] = between_type\n",
        "    config['fc']['fc_num'] = fc_num\n",
        "    config['batch_size'] = batch_size\n",
        "\n",
        "    base = BASE_MODEL_NAME\n",
        "\n",
        "    base += '_resize_{}'.format(config['aug']['resize'][0])\n",
        "\n",
        "    base += '_conv_{}'.format('-'.join(map(lambda x:str(x),config['conv']['conv_num'])))\n",
        "    base += '_basech_{}'.format(config['conv']['base_channel'])\n",
        "    base += '_act_{}'.format(config['activation'])\n",
        "    base += '_pool_{}'.format(config['pool']['type'])\n",
        "    base += '_betw_{}'.format(config['between_type'])\n",
        "    base += '_fc_{}'.format(config['fc']['fc_num'])\n",
        "    base += '_zscore_{}'.format(config['is_zscore'])\n",
        "    base += '_batch_{}'.format(config['batch_size'])\n",
        "    if config['is_dropout']:\n",
        "        base += '_DO_'+str(config['dropout_rate']).replace('.', '')\n",
        "    if config['is_batchnorm']:\n",
        "        base += '_BN'+'_O'\n",
        "    else:\n",
        "        base += '_BN'+'_X'\n",
        "\n",
        "    model_name = base\n",
        "    print(model_name)\n",
        "\n",
        "    ### Define dataset\n",
        "    dataset = tf.data.TFRecordDataset(train_tfrecord_path, compression_type='GZIP')\n",
        "    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # dataset = dataset.cache()\n",
        "    dataset = dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.map(resize_and_crop_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.map(image_aug_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(config['buffer_size'])\n",
        "    dataset = dataset.batch(config['batch_size'])\n",
        "    dataset = dataset.map(post_process_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    val_dataset = tf.data.TFRecordDataset(val_tfrecord_path, compression_type='GZIP')\n",
        "    val_dataset = val_dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    val_dataset = val_dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    val_dataset = val_dataset.map(resize_and_crop_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # val_dataset = val_dataset.map(image_aug_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # val_dataset = val_dataset.shuffle(config['buffer_size'])\n",
        "    val_dataset = val_dataset.batch(config['batch_size'])\n",
        "    val_dataset = val_dataset.map(post_process_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # val_dataset = val_dataset.cache()\n",
        "    val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    model_path = pth.join(\n",
        "        model_checkpoint_path, model_name, \n",
        "    )\n",
        "    model = build_cnn(config)\n",
        "    #         model.summary()\n",
        "    model.compile(loss=config['loss'], optimizer=Adam(lr=config['learning_rate']),\n",
        "                  metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
        "    initial_epoch = 0\n",
        "\n",
        "    if pth.isdir(model_path) and len([_ for _ in os.listdir(model_path) if _.endswith('hdf5')]) >= 1:\n",
        "        model_chk_name = sorted(os.listdir(model_path))[-1]\n",
        "        initial_epoch = int(model_chk_name.split('-')[0])\n",
        "        model.load_weights(pth.join(model_path, model_chk_name))\n",
        "\n",
        "    # ### Freeze first layer\n",
        "    # conv_list = [layer for layer in model.layers if isinstance(layer, keras.layers.Conv2D)]\n",
        "    # conv_list[0].trainable = False\n",
        "    # # conv_list[1].trainable = False\n",
        "\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    model_filename = pth.join(model_path, '{epoch:06d}-{val_loss:0.6f}-{loss:0.6f}.hdf5')\n",
        "    checkpointer = ModelCheckpoint(\n",
        "        filepath=model_filename, verbose=1, \n",
        "        period=1, save_best_only=True, \n",
        "        monitor='val_loss'\n",
        "    )\n",
        "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
        "\n",
        "    hist = model.fit(\n",
        "        x=dataset, epochs=config['num_epoch'], \n",
        "        validation_data=val_dataset, shuffle=True,\n",
        "        callbacks = [checkpointer, es], #batch_size=config['batch_size']\n",
        "        initial_epoch=initial_epoch,\n",
        "        # steps_per_epoch=train_num_steps, validation_steps=val_num_steps,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    model_analysis_path = model_path.replace('checkpoint', 'analysis')\n",
        "    visualization_path = pth.join(model_analysis_path,'visualization')\n",
        "    os.makedirs(visualization_path, exist_ok=True)\n",
        "    \n",
        "    print()\n",
        "    # clear_output()        \n",
        "    for each_label in ['loss', 'acc', 'precision', 'recall', 'auc']:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(hist.history[each_label], 'g', label='train_{}'.format(each_label))\n",
        "        ax.plot(hist.history['val_{}'.format(each_label)], 'r', label='val_{}'.format(each_label))\n",
        "        ax.set_xlabel('epoch')\n",
        "        ax.set_ylabel('loss')\n",
        "        ax.legend(loc='upper left')\n",
        "        if not each_label == 'loss':\n",
        "            plt.ylim(0, 1)\n",
        "        plt.show()\n",
        "        filename = 'learning_curve_{}'.format(each_label)\n",
        "#             fig.savefig(pth.join(visualization_path, filename), transparent=True)\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        plt.close('all')\n",
        "\n",
        "    np.savez_compressed(pth.join(visualization_path, 'learning_curve'), \n",
        "                        loss=hist.history['loss'], \n",
        "                        val_loss=hist.history['val_loss'],\n",
        "                        acc=hist.history['acc'], \n",
        "                        val_acc=hist.history['val_acc'],\n",
        "                        precision=hist.history['precision'], \n",
        "                        vaval_precisionl_mae=hist.history['val_precision'],  \n",
        "                        recall=hist.history['recall'],\n",
        "                        val_recall=hist.history['val_recall'],\n",
        "                        auc=hist.history['auc'],\n",
        "                        val_auc=hist.history['val_auc']\n",
        "                        )\n",
        "\n",
        "    model.save(pth.join(model_path, '000000_last.hdf5'))\n",
        "    K.clear_session()\n",
        "    del(model)\n",
        "    \n",
        "    model_analysis_base_path = pth.join(model_base_path, 'analysis', model_name) \n",
        "    with open(pth.join(model_analysis_base_path, 'config.json'), 'w') as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "    chk_name_list = sorted([name for name in os.listdir(model_path) if name != '000000_last.hdf5'])\n",
        "    for chk_name in chk_name_list[:-2]:\n",
        "        os.remove(pth.join(model_path, chk_name))\n",
        "    # clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibXfENT5zvwZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ARllmjWGk-"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1S7DrvwhFPM"
      },
      "source": [
        "image_feature_description_for_test = {\n",
        "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    # 'randmark_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    # 'id': tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "def _parse_image_function_for_test(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, image_feature_description_for_test)\n",
        "\n",
        "def map_func_for_test(target_record):\n",
        "    img = target_record['image_raw']\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.dtypes.cast(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "def resize_and_crop_func_for_test(image):\n",
        "    result_image = tf.image.resize(image, config['aug']['resize'])\n",
        "    # result_image = tf.image.random_crop(image, size=config['input_shape'], seed=7777)\n",
        "    return result_image\n",
        "\n",
        "def post_process_func_for_test(image):\n",
        "    # result_image = result_image / 255\n",
        "    result_image = my_model_base.preprocess_input(image)\n",
        "    return result_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi2igBp6WSYD"
      },
      "source": [
        "submission_base_path = pth.join(data_base_path, 'submission')\n",
        "os.makedirs(submission_base_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71z9_wKaMPTJ"
      },
      "source": [
        "for conv_comb, activation, base_channel, \\\n",
        "    between_type, fc_num, batch_size \\\n",
        "        in itertools.product(conv_comb_list, activation_list,\n",
        "                              base_channel_list, between_type_list, fc_list,\n",
        "                              batch_size_list):\n",
        "    config['conv']['conv_num'] = conv_comb\n",
        "    config['conv']['base_channel'] = base_channel\n",
        "    config['activation'] = activation\n",
        "    config['between_type'] = between_type\n",
        "    config['fc']['fc_num'] = fc_num\n",
        "    config['batch_size'] = batch_size\n",
        "\n",
        "    base = BASE_MODEL_NAME\n",
        "\n",
        "    base += '_resize_{}'.format(config['aug']['resize'][0])\n",
        "\n",
        "    base += '_conv_{}'.format('-'.join(map(lambda x:str(x),config['conv']['conv_num'])))\n",
        "    base += '_basech_{}'.format(config['conv']['base_channel'])\n",
        "    base += '_act_{}'.format(config['activation'])\n",
        "    base += '_pool_{}'.format(config['pool']['type'])\n",
        "    base += '_betw_{}'.format(config['between_type'])\n",
        "    base += '_fc_{}'.format(config['fc']['fc_num'])\n",
        "    base += '_zscore_{}'.format(config['is_zscore'])\n",
        "    base += '_batch_{}'.format(config['batch_size'])\n",
        "    if config['is_dropout']:\n",
        "        base += '_DO_'+str(config['dropout_rate']).replace('.', '')\n",
        "    if config['is_batchnorm']:\n",
        "        base += '_BN'+'_O'\n",
        "    else:\n",
        "        base += '_BN'+'_X'\n",
        "\n",
        "    model_name = base\n",
        "    print(model_name)\n",
        "\n",
        "    ### Define dataset\n",
        "    test_dataset = tf.data.TFRecordDataset(test_tfrecord_path, compression_type='GZIP')\n",
        "    test_dataset = test_dataset.map(_parse_image_function_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.map(map_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.map(resize_and_crop_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.batch(config['batch_size'])\n",
        "    test_dataset = test_dataset.map(post_process_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    model_path = pth.join(\n",
        "        model_checkpoint_path, model_name, \n",
        "    )\n",
        "    model = build_cnn(config)\n",
        "    #         model.summary()\n",
        "    model.compile(loss=config['loss'], optimizer=Adam(lr=config['learning_rate']),\n",
        "                  metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
        "    initial_epoch = 0\n",
        "\n",
        "    model_chk_name = sorted(os.listdir(model_path))[-1]\n",
        "    initial_epoch = int(model_chk_name.split('-')[0])\n",
        "    model.load_weights(pth.join(model_path, model_chk_name))\n",
        "\n",
        "    preds = model.predict(test_dataset, verbose=1)\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    pred_probs = np.array([pred[indice] for pred, indice in zip(preds, pred_labels)])\n",
        "\n",
        "    submission_csv_path = pth.join(data_base_path, submission_csv_name)\n",
        "    submission_df = pd.read_csv(submission_csv_path)\n",
        "\n",
        "    submission_df['landmark_id'] = pred_labels\n",
        "    submission_df['conf'] = pred_probs\n",
        "\n",
        "    today_str = datetime.date.today().strftime('%Y%m%d')\n",
        "    result_filename = '{}.csv'.format(model_name)\n",
        "    submission_csv_fileaname = pth.join(submission_base_path, '_'.join([today_str, result_filename]))\n",
        "    submission_df.to_csv(submission_csv_fileaname, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMStwUj7nYz9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}